{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_slice\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('max_rows', 200)\n",
    "pd.set_option('display.max_columns', 300)\n",
    "np.random.seed(666)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:20,.5f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = 'diabetes_mellitus'\n",
    "cat_cols = ['ethnicity','hospital_admit_source','icu_admit_source','icu_stay_type','icu_type',\n",
    "            'age_group','demo_profile','apache_3j_group','a1c_bp_group',\n",
    "            'apache_2_diagnosis','icu_id1','icu_id2']\n",
    "k = 5\n",
    "seed = 8\n",
    "\n",
    "def target_encode(var, dv, weight = 36):\n",
    "    mean = dv.mean()\n",
    "    true_weight = weight * max(1, mean/(1-mean))\n",
    "    agg = dv.groupby(var,dropna=False).agg(['count','mean'])\n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "    return (counts * means + true_weight) / (counts + true_weight/mean)\n",
    "\n",
    "def target_encode_train_test(var_train, dv_train, var_test, weight = 36):\n",
    "    encode_key = target_encode(var_train,dv_train,weight)\n",
    "    return var_train.map(encode_key), var_test.map(encode_key).astype(np.float32).fillna(dv_train.mean())\n",
    "\n",
    "def reduce_mem_usage(df: pd.DataFrame,\n",
    "                     verbose: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "\n",
    "                if (c_min > np.iinfo(np.int32).min\n",
    "                      and c_max < np.iinfo(np.int32).max):\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif (c_min > np.iinfo(np.int64).min\n",
    "                      and c_max < np.iinfo(np.int64).max):\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (c_min > np.finfo(np.float32).min\n",
    "                      and c_max < np.finfo(np.float32).max):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    reduction = (start_mem - end_mem) / start_mem\n",
    "\n",
    "    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
    "    if verbose:\n",
    "        print(msg)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train_FE2_filter5.csv')\n",
    "test = pd.read_csv('Data/test_FE2_filter5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dataset_label'] = 0\n",
    "test['dataset_label'] = 1\n",
    "test[TARGET_COL] = np.nan\n",
    "combined = pd.concat([train, test], ignore_index=True)\n",
    "combined['icu_id1'] = np.where(combined.icu_id <= 180, combined.icu_id, np.nan)\n",
    "combined['icu_id2'] = np.where(combined.icu_id > 180, combined.icu_id, np.nan)\n",
    "str_cols = ['icu_id1','icu_id2','apache_2_diagnosis']\n",
    "combined[str_cols] = combined[str_cols].astype(str)\n",
    "combined[cat_cols] = combined[cat_cols].fillna('')\n",
    "cat_encoder = OrdinalEncoder(dtype=np.int).fit(combined[cat_cols])\n",
    "combined[cat_cols] = cat_encoder.transform(combined[cat_cols])\n",
    "train = combined[combined.dataset_label == 0].drop('dataset_label',axis=1)\n",
    "test = combined[combined.dataset_label == 1].drop(['dataset_label',TARGET_COL],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 250.52 MB (49.1 % reduction)\n",
      "Mem. usage decreased to 19.67 MB (49.1 % reduction)\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_ignore = ['encounter_id','hospital_id']\n",
    "vars_to_encode = ['icu_id','apache_3j_diagnosis','demo_profile']\n",
    "trainX = train.drop(features_to_ignore+[TARGET_COL],axis=1)\n",
    "trainY = train[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, k=5, repeats=5, seed=666, device='GPU'):\n",
    "    aucs = np.empty(k*repeats)\n",
    "    kf = RepeatedKFold(n_splits=k, n_repeats=repeats, random_state=seed)    \n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(trainX,trainY)):\n",
    "        trainX_cv, trainY_cv = trainX.iloc[train_index,:], trainY[train_index]\n",
    "        validX_cv, validY_cv = trainX.iloc[valid_index,:], trainY[valid_index]\n",
    "\n",
    "        for col in vars_to_encode:\n",
    "            trainX_cv[col+'_encoded'], validX_cv[col+'_encoded'] = target_encode_train_test(\n",
    "                trainX_cv[col], trainY_cv, validX_cv[col])\n",
    "\n",
    "        for col in ['icu_id1','icu_id2']:\n",
    "            train_only = list(set(trainX_cv[col].unique()) - set(validX_cv[col].unique()))\n",
    "            valid_only = list(set(validX_cv[col].unique()) - set(trainX_cv[col].unique()))\n",
    "            trainX_cv.loc[trainX_cv[col].isin(train_only), col] = np.nan\n",
    "            validX_cv.loc[validX_cv[col].isin(valid_only), col] = np.nan\n",
    "\n",
    "        trainX_cv.drop(['apache_3j_diagnosis','icu_id'],axis=1,inplace=True)\n",
    "        validX_cv.drop(['apache_3j_diagnosis','icu_id'],axis=1,inplace=True)\n",
    "        lgtrain = lgb.Dataset(trainX_cv, label=trainY_cv, categorical_feature=cat_cols)\n",
    "        lgvalid = lgb.Dataset(validX_cv, label=validY_cv, categorical_feature=cat_cols)\n",
    "        params = {\n",
    "            'objective':'binary'\n",
    "            ,'boosting': 'rf'\n",
    "            ,'verbosity': -1\n",
    "            ,'metric': 'AUC'\n",
    "            ,'learning_rate': .04\n",
    "            ,'early_stopping_rounds': 100\n",
    "            ,'device': device\n",
    "            ,'seed': 87707\n",
    "            ,'bagging_freq': 1\n",
    "            ,'bagging_fraction': .2\n",
    "            ,'feature_fraction': .8\n",
    "            ,'num_leaves': 191\n",
    "            ,'min_data_in_leaf': 50\n",
    "            ,'lambda_l2': trial.suggest_uniform('lambda_l2',0,100)\n",
    "        }        \n",
    "        \n",
    "        model_cv = lgb.train(params, lgtrain, num_boost_round=2000,\n",
    "                             valid_sets=lgvalid, verbose_eval=-1)\n",
    "        preds = model_cv.predict(validX_cv)\n",
    "        aucs[i] = roc_auc_score(validY_cv, preds)\n",
    "    return aucs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-02-28 18:15:19,366]\u001b[0m A new study created in memory with name: no-name-29c887e0-f93b-446c-8026-0577e58d0c3e\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[461]\tvalid_0's auc: 0.858654\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[266]\tvalid_0's auc: 0.85955\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[215]\tvalid_0's auc: 0.855642\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[261]\tvalid_0's auc: 0.859815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-02-28 18:18:03,773]\u001b[0m Trial 0 finished with value: 0.858402167093137 and parameters: {'lambda_l2': 0.2}. Best is trial 0 with value: 0.858402167093137.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    'lambda_l2': [.2]\n",
    "}\n",
    "#no extratrees - 0.8584292818953829\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',sampler=optuna.samplers.GridSampler(search_space))\n",
    "\n",
    "# study = optuna.create_study(\n",
    "#     direction='maximize',sampler=optuna.samplers.RandomSampler())\n",
    "# study = optuna.create_study(\n",
    "#     direction='maximize',\n",
    "#     sampler=optuna.samplers.CmaEsSampler(n_startup_trials=10, consider_pruned_trials=False),\n",
    "#     pruner=optuna.pruners.HyperbandPruner()\n",
    "# )\n",
    "study.optimize(lambda trial: objective(trial, k=4, repeats=1, seed=87007, device='GPU'), n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(study, \"lgbm_200iter3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
