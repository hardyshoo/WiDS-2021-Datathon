{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some feature-engineering ideas were inspired by the following kernel: https://www.kaggle.com/siavrez/2020fatures\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "pd.set_option('max_rows', 300)\n",
    "pd.set_option('display.max_columns', 300)\n",
    "np.random.seed(666)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:20,.3f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df: pd.DataFrame,\n",
    "                     verbose: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "\n",
    "                if (c_min > np.iinfo(np.int32).min\n",
    "                      and c_max < np.iinfo(np.int32).max):\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif (c_min > np.iinfo(np.int64).min\n",
    "                      and c_max < np.iinfo(np.int64).max):\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (c_min > np.finfo(np.float32).min\n",
    "                      and c_max < np.finfo(np.float32).max):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    reduction = (start_mem - end_mem) / start_mem\n",
    "\n",
    "    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
    "    if verbose:\n",
    "        print(msg)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"diabetes_mellitus\"\n",
    "RATIO_CAP = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data and data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"Raw_Data/TrainingWiDS2021.csv\")\n",
    "test_raw = pd.read_csv(\"Raw_Data/UnlabeledWiDS2021.csv\")\n",
    "dd = pd.read_csv(\"Raw_Data/DataDictionaryWiDS2021.csv\")\n",
    "age_bmi_chart = pd.read_csv('age_bmi_chart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_raw.drop('Unnamed: 0',axis=1)\n",
    "test = test_raw.drop('Unnamed: 0',axis=1)\n",
    "dd = dd[dd['Variable Name'] != 'icu_admit_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine train and test sets to apply operations consistently across both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dataset_label'] = 0\n",
    "test['dataset_label'] = 1\n",
    "test[TARGET_COL] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([train,test],ignore_index=True)\n",
    "combined['hospital_admit_source'] = combined['hospital_admit_source'].replace({\n",
    "    'Other ICU':'ICU','ICU to SDU':'SDU', 'Step-Down Unit (SDU)': 'SDU',\n",
    "    'Other Hospital':'Other','Observation': 'Recovery Room'})\n",
    "binary_vars = dd['Variable Name'][dd['Data Type'] == 'binary']\n",
    "bool_vars = combined[binary_vars].isna().sum().index[combined[binary_vars].isna().sum() == 0]\n",
    "combined[bool_vars] = combined[bool_vars].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if NA values consistently appear between min and max values (they should both be NA or both not be NA)\n",
    "for col in [col for col in combined.columns if col.endswith('_min')]:\n",
    "    col_max = re.sub('_min$','_max',col)\n",
    "    if any(combined[[col,col_max]].isna().sum(axis=1) == 1):\n",
    "        print(col)\n",
    "\n",
    "# checking if NA values consistently appear between h1 and d1 values (if h1 value is NA then d1 value should also be NA)\n",
    "for col in [col for col in combined.columns if col.startswith('d1_')]:\n",
    "    col_h1 = re.sub('^d1_','h1_',col)\n",
    "    if any(combined[col].isna() & combined[col_h1].notna()):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tests = [re.sub('_min$','',re.sub('^d1_','',col)) for col in combined.columns if col.startswith('d1_') and col.endswith('_min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders = pd.DataFrame()\n",
    "# for test in lab_tests:\n",
    "#     test_d1_min = 'd1_'+test+'_min'\n",
    "#     test_d1_max = 'd1_'+test+'_max'\n",
    "#     test_h1_min = 'h1_'+test+'_min'\n",
    "#     test_h1_max = 'h1_'+test+'_max'\n",
    "#     ranks = pd.DataFrame(combined[[\n",
    "#         test_d1_min,test_h1_min,test_h1_max,test_d1_max]].dropna().rank(axis=1,method='first').values)\n",
    "#     orders = pd.concat([orders,ranks],ignore_index=True)\n",
    "# orders = orders.astype(int).astype(str)\n",
    "# (orders[0]+orders[1]+orders[2]+orders[3]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching min and max values if min > max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in lab_tests:\n",
    "    d1_min_test = 'd1_'+t+'_min'\n",
    "    d1_max_test = 'd1_'+t+'_max'\n",
    "    h1_min_test = 'h1_'+t+'_min'\n",
    "    h1_max_test = 'h1_'+t+'_max'\n",
    "    new_d1_min_test = combined[[d1_min_test,d1_max_test]].min(axis=1)\n",
    "    new_d1_max_test = combined[[d1_min_test,d1_max_test]].max(axis=1)\n",
    "    new_h1_min_test = combined[[h1_min_test,h1_max_test]].min(axis=1)\n",
    "    new_h1_max_test = combined[[h1_min_test,h1_max_test]].max(axis=1)\n",
    "    combined[t+'_d1_max_min_switched'] = (\n",
    "        combined[d1_min_test] > new_d1_min_test) | (combined[d1_max_test] < new_d1_max_test)\n",
    "    combined[t+'_h1_max_min_switched'] = (\n",
    "        combined[h1_min_test] > new_h1_min_test) | (combined[h1_max_test] < new_h1_max_test)\n",
    "    combined[t+'_any_max_min_switched'] = (combined[t+'_d1_max_min_switched'] | combined[t+'_h1_max_min_switched'])\n",
    "    combined[d1_min_test] = new_d1_min_test\n",
    "    combined[d1_max_test] = new_d1_max_test\n",
    "    combined[h1_min_test] = new_h1_min_test\n",
    "    combined[h1_max_test] = new_h1_max_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = np.abs(combined.rank().corr())\n",
    "# high_corrs_dict = {}\n",
    "# corr_list = []\n",
    "# for index, row in corr_matrix.iterrows():\n",
    "#     corr_matrix.loc[index,index] = 0\n",
    "#     corr_vars = list(row.index[row > .99])\n",
    "#     if corr_vars:\n",
    "#         high_corrs_dict[index] = corr_vars\n",
    "#         corr_list.append(index)\n",
    "\n",
    "# corr_matrix[corr_matrix != 1].max().sort_values(ascending=False)\n",
    "# high_corrs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing value count features for each category of variables as identified by the data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute missing count variables\n",
    "categories = dd.Category.unique()\n",
    "categories = np.delete(categories, np.where(categories == 'identifier'))\n",
    "categories = np.delete(categories, np.where(categories == 'Target Variable'))\n",
    "for cat in categories:\n",
    "    combined['num_na_'+cat] = combined[dd['Variable Name'][dd.Category == cat]].isna().sum(axis=1)\n",
    "combined['num_na'] = combined.drop(TARGET_COL,axis=1).isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop high corr and nonunique columns\n",
    "extra_bp_vars = ['d1_diasbp_max','d1_diasbp_min','d1_sysbp_max','h1_sysbp_max','d1_sysbp_min','d1_mbp_min','h1_diasbp_max',\n",
    "                 'h1_mbp_max','h1_mbp_min','h1_sysbp_min','d1_mbp_max','h1_diasbp_min']\n",
    "combined = combined.drop(extra_bp_vars+['paco2_for_ph_apache','readmission_status'],axis=1)\n",
    "lab_tests = list(set(lab_tests) - {'diasbp','sysbp','mbp'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute age, weight, and height using MICE, with separate imputations for males and females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.loc[:,'age'].replace(0,np.nan,inplace=True)\n",
    "imputerM = IterativeImputer(initial_strategy='median').fit(combined[['age','weight','height']][combined.gender == 'M'])\n",
    "imputerF = IterativeImputer(initial_strategy='median').fit(combined[['age','weight','height']][combined.gender == 'F'])\n",
    "combined[['age_Mimputed','weight_Mimputed','height_Mimputed']] = imputerM.transform(combined[['age','weight','height']])\n",
    "combined[['age_Fimputed','weight_Fimputed','height_Fimputed']] = imputerF.transform(combined[['age','weight','height']])\n",
    "combined['age'] = np.where(combined['age'].notna(), combined['age'],\n",
    "                        np.where(combined['gender'] == 'M', combined['age_Mimputed'],\n",
    "                        np.where(combined['gender'] == 'F', combined['age_Fimputed'],\n",
    "                        np.nan)))\n",
    "combined['weight'] = np.where(combined['weight'].notna(), combined['weight'],\n",
    "                        np.where(combined['gender'] == 'M', combined['weight_Mimputed'],\n",
    "                        np.where(combined['gender'] == 'F', combined['weight_Fimputed'],\n",
    "                        np.nan)))\n",
    "combined['height'] = np.where(combined['height'].notna(), combined['height'],\n",
    "                        np.where(combined['gender'] == 'M', combined['height_Mimputed'],\n",
    "                        np.where(combined['gender'] == 'F', combined['height_Fimputed'],\n",
    "                        np.nan)))\n",
    "rows_to_drop = (combined[['age','weight','height']].isna().sum(axis=1) > 0)\n",
    "if len(combined[rows_to_drop & combined.dataset_label == 1]) == 0:\n",
    "    combined = combined[~rows_to_drop]\n",
    "combined.drop(['age_Mimputed','weight_Mimputed','height_Mimputed','age_Fimputed','weight_Fimputed','height_Fimputed'],\n",
    "              axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features based on blood pressure tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysbp_tests = combined.columns[combined.columns.str.contains('sysbp') & ~combined.columns.str.endswith('_switched')]\n",
    "for t in sysbp_tests:\n",
    "    diasbp_test = re.sub('sysbp','diasbp',t)\n",
    "    combined[re.sub('sysbp','sysbp_diasbp',t)+'_diff'] = combined[t] - combined[diasbp_test]\n",
    "    combined[re.sub('sysbp','sysbp_diasbp',t)+'_ratio'] = np.maximum(\n",
    "        np.minimum(combined[t] / combined[diasbp_test], RATIO_CAP), 0)\n",
    "    combined[re.sub('sysbp','sysbp_diasbp',t)+'_diff_0_or_less'] = combined[re.sub('sysbp','sysbp_diasbp',t)+'_diff'] <= 0\n",
    "for t in ['d1_sysbp_diasbp_invasive_max_diff','d1_sysbp_diasbp_noninvasive_max_diff',\n",
    "          'h1_sysbp_diasbp_invasive_max_diff','h1_sysbp_diasbp_noninvasive_max_diff']:\n",
    "    t_min = re.sub('_max_','_min_',t)\n",
    "    combined[re.sub('_diff','_min_range_diff',t)] = combined[t] - combined[t_min]\n",
    "    combined[re.sub('_diff','_min_range_ratio',t)] = np.maximum(np.minimum(combined[t] / combined[t_min], RATIO_CAP), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features based on logical operations on lab test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})\n",
    "for t in lab_tests:\n",
    "    d1_min_test = 'd1_'+t+'_min'\n",
    "    d1_max_test = 'd1_'+t+'_max'\n",
    "    h1_min_test = 'h1_'+t+'_min'\n",
    "    h1_max_test = 'h1_'+t+'_max'\n",
    "    combined[t+'_d1_max_min_diff'] = combined[d1_max_test] - combined[d1_min_test]\n",
    "    combined[t+'_h1_max_min_diff'] = combined[h1_max_test] - combined[h1_min_test]\n",
    "    combined[t+'_d1_max_min_ratio'] = np.maximum(np.minimum(combined[d1_max_test] / combined[d1_min_test], RATIO_CAP), 0)\n",
    "    combined[t+'_h1_max_min_ratio'] = np.maximum(np.minimum(combined[h1_max_test] / combined[h1_min_test], RATIO_CAP), 0)\n",
    "    combined[t+'_d1_avg'] = (combined[d1_max_test] + combined[d1_min_test])/2\n",
    "    combined[t+'_h1_avg'] = (combined[h1_max_test] + combined[h1_min_test])/2\n",
    "    combined[t+'_max_d1_h1_diff'] = combined[d1_max_test] - combined[h1_max_test]\n",
    "    combined[t+'_min_d1_h1_diff'] = combined[h1_min_test] - combined[d1_min_test]\n",
    "    combined[t+'_max_d1_h1_ratio'] = np.maximum(np.minimum(combined[d1_max_test] / combined[h1_max_test], RATIO_CAP), 0)\n",
    "    combined[t+'_min_d1_h1_ratio'] = np.maximum(np.minimum(combined[h1_min_test] / combined[d1_min_test], RATIO_CAP), 0)\n",
    "    combined[t+'_max_h1_more_extreme'] = combined[t+'_max_d1_h1_diff'] < 0\n",
    "    combined[t+'_min_h1_more_extreme'] = combined[t+'_min_d1_h1_diff'] < 0\n",
    "    combined[t+'_any_h1_more_extreme'] = (combined[t+'_max_h1_more_extreme'] | combined[t+'_min_h1_more_extreme'])\n",
    "    combined[t+'_d1_max_min_equal'] = (combined[t+'_d1_max_min_diff'] == 0)\n",
    "    combined[t+'_h1_max_min_equal'] = (combined[t+'_h1_max_min_diff'] == 0)\n",
    "    combined[t+'_max_d1_h1_equal'] = (combined[t+'_max_d1_h1_diff'] == 0)\n",
    "    combined[t+'_min_d1_h1_equal'] = (combined[t+'_min_d1_h1_diff'] == 0)\n",
    "    combined[t+'_max_or_min_d1_h1_equal'] = (combined[t+'_max_d1_h1_equal'] | combined[t+'_min_d1_h1_equal'])    \n",
    "    combined[t+'_d1_h1_range_diff'] = combined[t+'_d1_max_min_diff'] - combined[t+'_h1_max_min_diff']\n",
    "    combined[t+'_d1_h1_range_ratio'] = np.maximum(np.minimum(\n",
    "        combined[t+'_d1_max_min_diff'] / combined[t+'_h1_max_min_diff'], RATIO_CAP), 0)\n",
    "    combined[t+'_taken_d1'] = combined[[d1_min_test,d1_max_test]].notna().any(axis=1)\n",
    "    combined[t+'_taken_h1'] = combined[[h1_min_test,h1_max_test]].notna().any(axis=1)\n",
    "    combined[t+'_taken_only_after_h1'] = combined[t+'_taken_d1'] & ~combined[t+'_taken_h1']\n",
    "    apache_test = t+'_apache'\n",
    "    if apache_test in combined.columns:\n",
    "        combined[t+'_d1_max_apache_diff'] = combined[d1_max_test] - combined[apache_test]\n",
    "        combined[t+'_d1_min_apache_diff'] = combined[apache_test] - combined[d1_min_test]\n",
    "        combined[t+'_d1_max_apache_ratio'] = np.maximum(np.minimum(combined[d1_max_test] / combined[apache_test], 2), 0)\n",
    "        combined[t+'_d1_min_apache_ratio'] = np.maximum(np.minimum(combined[apache_test] / combined[d1_min_test], 2), 0)\n",
    "        combined[t+'_d1_max_apache_equal_or_greater'] = (combined[t+'_d1_max_apache_diff'] <= 0)\n",
    "        combined[t+'_d1_min_apache_equal_or_lesser'] = (combined[t+'_d1_min_apache_diff'] <= 0)\n",
    "combined['any_max_min_switched'] = combined[combined.columns[combined.columns.str.endswith('_max_min_switched')]].any(axis=1)\n",
    "combined['any_h1_more_extreme'] = combined[combined.columns[combined.columns.str.endswith('_h1_more_extreme')]].any(axis=1)\n",
    "combined['num_d1_tests'] = combined[combined.columns[combined.columns.str.endswith('_taken_d1')]].sum(axis=1)\n",
    "combined['num_h1_tests'] = combined[combined.columns[combined.columns.str.endswith('_taken_h1')]].sum(axis=1)\n",
    "combined['num_d1_not_h1_tests'] = combined[combined.columns[combined.columns.str.endswith('_taken_only_after_h1')]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create age and weight categories, and combine with ethnicity to create a demographic grouping variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['bmi'] = combined['weight']/((combined['height']/100)**2)\n",
    "combined['age_range'] = pd.cut(combined.age, bins=[combined.age.min()-1,25,30,35,40,45,50,55,60,65,70,75,combined.age.max()+1], \n",
    "       right=False, labels=False)\n",
    "combined = combined.merge(age_bmi_chart.drop('Age_Range',axis=1),left_on='age_range',right_on='Age_Index',how='left')\n",
    "combined['weight_category'] = np.where(combined['bmi'] < combined['Percentile5'], 'Underweight',\n",
    "                                    np.where(combined['bmi'] <= combined['Percentile85'], 'Healthy',\n",
    "                                    np.where(combined['bmi'] <= combined['Percentile95'], 'Overweight',\n",
    "                                    np.where(combined['bmi'] > combined['Percentile95'], 'Obese',''))))\n",
    "combined['age_group'] = np.where(combined['age'] < 30, '18to29',\n",
    "                            np.where(combined['age'] < 45, '30to44', \n",
    "                            np.where(combined['age'] < 65, '45to64',\n",
    "                            np.where(combined['age'] >= 65, '65+', ''))))\n",
    "combined['ethnic_group'] = np.where(combined.ethnicity == 'Caucasian', 'caucasian',\n",
    "                                np.where(combined.ethnicity=='African American','african_american','other'))\n",
    "combined['demo_profile'] = (combined['age_group'] + '_' + combined['weight_category'] + '_' + \n",
    "                            combined['ethnic_group'] + '_' + combined['gender'])\n",
    "combined.drop(['ethnic_group','age_range','Age_Index','Percentile5','Percentile25','Percentile50',\n",
    "               'Percentile75','Percentile85','Percentile95'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create grouping variables for glucose and blood pressure, as these are risk factors for diabetes, plus some misc. features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['apache_3j_group'] = np.where(combined['apache_3j_diagnosis'].isna(), '',\n",
    "                                    np.where(combined['apache_3j_diagnosis'] < 200, 'Cardiovascular' , \n",
    "                                    np.where(combined['apache_3j_diagnosis'] < 400, 'Respiratory' , \n",
    "                                    np.where(combined['apache_3j_diagnosis'] < 500, 'Neurological' , \n",
    "                                    np.where(combined['apache_3j_diagnosis'] < 600, 'Sepsis' , \n",
    "                                    np.where(combined['apache_3j_diagnosis'] < 800, 'Trauma' ,  \n",
    "                                    np.where(combined['apache_3j_diagnosis'] < 900, 'Haematological' ,         \n",
    "                                    np.where(combined['apache_3j_diagnosis'] < 1000, 'Renal/Genitourinary' ,         \n",
    "                                    np.where(combined['apache_3j_diagnosis'] < 1200, 'Musculoskeletal/Skin disease',\n",
    "                                             'Operative Sub-Diagnosis Codes' )))))))))\n",
    "combined['gcs_sum'] = (combined['gcs_eyes_apache']+combined['gcs_motor_apache']+combined['gcs_verbal_apache']).fillna(0)\n",
    "combined['comorbidity_score'] = (combined['aids'] * 23 + combined['cirrhosis'] * 4  + combined['hepatic_failure'] * 16 + \n",
    "                                 combined['immunosuppression'] * 10 + combined['leukemia'] * 10 + combined['lymphoma'] * 13 + \n",
    "                                 combined['solid_tumor_with_metastasis'] * 11).fillna(0)\n",
    "combined['a1c'] = ((combined['glucose_d1_avg'])/2 + 46.7)/28.7\n",
    "combined['a1c_group'] = np.where(combined['a1c'] < 5.7, 'Normal',\n",
    "                                np.where(combined['a1c'] < 6.5, 'Prediabetes',\n",
    "                                np.where(combined['a1c'] >= 6.5, 'Diabetes', 'Not_Measured')))\n",
    "combined.drop('a1c',axis=1,inplace=True)\n",
    "combined['bp_group'] = np.where(combined.sysbp_invasive_d1_avg.isna() &\n",
    "                                combined.sysbp_noninvasive_d1_avg.isna() &\n",
    "                                combined.diasbp_noninvasive_d1_avg.isna() &\n",
    "                                combined.diasbp_invasive_d1_avg.isna(), 'Not_Measured',\n",
    "                            np.where((combined.sysbp_invasive_d1_avg >= 140) | \n",
    "                                (combined.sysbp_noninvasive_d1_avg >= 140) |\n",
    "                                (combined.diasbp_invasive_d1_avg >= 90) | \n",
    "                                (combined.diasbp_noninvasive_d1_avg >= 90), 'Stage_2_Hypertension',\n",
    "                            np.where((combined.sysbp_invasive_d1_avg >= 130) | \n",
    "                                (combined.sysbp_noninvasive_d1_avg >= 130) |\n",
    "                                (combined.diasbp_invasive_d1_avg >= 80) | \n",
    "                                (combined.diasbp_noninvasive_d1_avg >= 80), 'Stage_1_Hypertension',\n",
    "                            np.where((combined.sysbp_invasive_d1_avg < 90) | \n",
    "                                (combined.sysbp_noninvasive_d1_avg < 90) |\n",
    "                                (combined.diasbp_invasive_d1_avg < 60) | \n",
    "                                (combined.diasbp_noninvasive_d1_avg < 60), 'Hypotension',\n",
    "                            np.where((combined.sysbp_invasive_d1_avg >= 120) | \n",
    "                                (combined.sysbp_noninvasive_d1_avg >= 120), 'Eleveated_BP','Normal_BP')))))\n",
    "combined['a1c_bp_group'] = np.where(combined['bp_group'] == 'Not_Measured', 'NoBP',\n",
    "                                    combined['a1c_group'] + '_' + combined['bp_group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create aggregated mean and index lab test features for each grouping feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouper in ['apache_2_diagnosis','demo_profile','a1c_bp_group']:\n",
    "    for lab in list(combined.columns[combined.columns.str.endswith('avg')])+['weight','bmi','age']:\n",
    "        mean_name = '{0}_{1}_mean'.format(grouper,lab)\n",
    "        combined[mean_name] = combined[grouper].map(combined.groupby(grouper)[lab].mean())\n",
    "        combined[mean_name+'_diff'.format(grouper,lab)] = combined[lab] - combined[mean_name]\n",
    "        combined[mean_name+'_ratio'.format(grouper,lab)] = np.maximum(np.minimum(\n",
    "            combined[lab] / combined[mean_name], RATIO_CAP), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency encoding of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['ethnicity','gender','hospital_admit_source','icu_admit_source','icu_stay_type','icu_type',\n",
    "            'weight_category','age_group','demo_profile','apache_3j_group','a1c_group','bp_group','a1c_bp_group',\n",
    "            'apache_2_diagnosis','apache_3j_diagnosis']\n",
    "for cat in cat_cols:\n",
    "    combined['{0}_freq'.format(cat)] = np.log1p(combined[cat].map(combined[cat].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_keep = list(pd.read_csv('Data/test_FE2_filter5.csv').columns)\n",
    "# cols_to_keep += ['dataset_label',TARGET_COL]\n",
    "# combined = reduce_mem_usage(combined[cols_to_keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up and split back into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and remove duplicate features\n",
    "dropped_cols = set(combined.columns) - set(combined.T.drop_duplicates().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = reduce_mem_usage(combined.drop(dropped_cols,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = combined[combined.dataset_label == 0].drop('dataset_label',axis=1)\n",
    "test_out = combined[combined.dataset_label == 1].drop(['dataset_label', TARGET_COL],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_out.shape)\n",
    "print(test_raw.shape)\n",
    "print(train_out.shape)\n",
    "print(train_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out.to_csv('Data/train_FE2_apache.csv',index=False)\n",
    "test_out.to_csv('Data/test_FE2_apache.csv',index=False)\n",
    "# train_out.to_csv('Data/train_FE2_filter5_ratio_cap4.csv',index=False)\n",
    "# test_out.to_csv('Data/test_FE2_filter5_ratio_cap4.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
