{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import metrics, losses, callbacks, optimizers, regularizers\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import copy\n",
    "# from datetime import datetime\n",
    "\n",
    "pd.set_option('max_rows', 500)\n",
    "pd.set_option('display.max_columns', 300)\n",
    "np.random.seed(666)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:20,.3f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'data0211_2_corr85_and_weight_removed_nn_default'\n",
    "TARGET_COL = 'diabetes_mellitus'\n",
    "cat_cols = ['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n",
    "cols_to_onehot = ['ethnicity', 'gender', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n",
    "vars_to_encode = ['icu_id','apache_3j_diagnosis','hospital_admit_source']\n",
    "vars_to_exclude = ['hospital_id','encounter_id','apache_2_diagnosis']\n",
    "\n",
    "def target_encode(var, dv, weight = 36):\n",
    "    mean = dv.mean()\n",
    "    true_weight = weight * max(1, mean/(1-mean))\n",
    "    agg = dv.groupby(var,dropna=False).agg(['count','mean'])\n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "    return (counts * means + true_weight) / (counts + true_weight/mean)\n",
    "\n",
    "def target_encode_train_test(var_train, dv_train, var_test, weight = 36):\n",
    "    encode_key = target_encode(var_train,dv_train,weight)\n",
    "    return var_train.map(encode_key), var_test.map(encode_key).fillna(dv_train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train_capped_ratio_all.csv')\n",
    "test = pd.read_csv('Data/test_capped_ratio_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns[train.columns.str.endswith('_ratio')]:\n",
    "    train[col], test[col] = np.log(train[col]), np.log(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot low-cardinality categoricals\n",
    "target = train[TARGET_COL].astype('bool')\n",
    "train[cols_to_onehot] = train[cols_to_onehot].fillna('missing')\n",
    "test[cols_to_onehot] = test[cols_to_onehot].fillna('missing')\n",
    "train_onehot = pd.get_dummies(train.drop(TARGET_COL,axis=1), columns = cols_to_onehot)\n",
    "test_onehot = pd.get_dummies(test, columns = cols_to_onehot)\n",
    "# train_onehot = train_onehot.replace(np.inf, np.nan)\n",
    "# test_onehot = test_onehot.replace(np.inf, np.nan)\n",
    "# TODO: Binary for missingness of numericals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_onehot, validX_onehot, trainY, validY = train_test_split(\n",
    "    train_onehot, target, test_size = .2, stratify = target, random_state=666)\n",
    "trainX_onehot = trainX_onehot.reset_index()\n",
    "validX_onehot = validX_onehot.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encode selected variables\n",
    "for col in vars_to_encode:\n",
    "    trainX_onehot[col+'_encoded'], validX_onehot[col+'_encoded'] = target_encode_train_test(\n",
    "        trainX_onehot[col],trainY,validX_onehot[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vars = list(trainX_onehot.columns[trainX_onehot.nunique() == 2] & trainX_onehot.columns[trainX_onehot.isna().sum() == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize nonbinary variables to 0 mean 1 SD\n",
    "trainX_to_scale = trainX_onehot.drop(binary_vars+vars_to_encode+vars_to_exclude,axis=1)\n",
    "validX_to_scale = validX_onehot.drop(binary_vars+vars_to_encode+vars_to_exclude,axis=1)\n",
    "trainX_scaler = StandardScaler().fit(trainX_to_scale)\n",
    "trainX_scaled = pd.DataFrame(trainX_scaler.transform(trainX_to_scale), columns = trainX_to_scale.columns)\n",
    "validX_scaled = pd.DataFrame(trainX_scaler.transform(validX_to_scale), columns = validX_to_scale.columns)\n",
    "trainX_scaled = trainX_scaled.fillna(0)\n",
    "validX_scaled = validX_scaled.fillna(0)\n",
    "trainX_ready = pd.concat([trainX_scaled, trainX_onehot[binary_vars]],axis=1)#[binary_vars]\n",
    "validX_ready = pd.concat([validX_scaled, validX_onehot[binary_vars]],axis=1)#[binary_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = len(trainX_ready.columns)\n",
    "base_units = np.floor(np.sqrt(num_cols))\n",
    "base_batch_size = int(np.power(2,np.floor(np.log2(np.sqrt(len(trainX_ready))))))\n",
    "tuning_num_trials = 60\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss'\n",
    "    ,min_delta=0.001\n",
    "    ,patience=9\n",
    "    ,verbose=1\n",
    "    ,mode='auto'\n",
    "    ,baseline=None\n",
    "    ,restore_best_weights=True\n",
    ")\n",
    "reduce_lr_on_plateau = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss'\n",
    "    ,factor=0.4\n",
    "    ,patience=3\n",
    "    ,verbose=1\n",
    "    ,mode='auto'\n",
    "    ,min_delta=0.001\n",
    "    ,cooldown=0\n",
    "    ,min_lr=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineModel = Sequential()\n",
    "baselineModel.add(Dense(\n",
    "    units = base_units\n",
    "    ,input_dim=num_cols\n",
    "    ,activation='relu'\n",
    "    ,use_bias=True\n",
    "    ,kernel_initializer='he_uniform'\n",
    "    ,bias_initializer='zeros'\n",
    "    ,kernel_regularizer=None\n",
    "    ,bias_regularizer=None\n",
    "    ,activity_regularizer=None\n",
    "    ,kernel_constraint=None\n",
    "    ,bias_constraint=None\n",
    "))\n",
    "baselineModel.add(Dense(\n",
    "    units=1\n",
    "    ,activation='sigmoid'\n",
    "))\n",
    "baselineModel.compile(\n",
    "    optimizer=optimizers.Adam()\n",
    "    ,loss=losses.BinaryCrossentropy()\n",
    "    ,metrics=['AUC']\n",
    "    ,loss_weights=None\n",
    "    ,weighted_metrics=None\n",
    "    ,run_eagerly=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 18)                5940      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 19        \n",
      "=================================================================\n",
      "Total params: 5,959\n",
      "Trainable params: 5,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baselineModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "392/392 - 1s - loss: 0.4353 - auc: 0.7854 - val_loss: 0.4040 - val_auc: 0.8250\n",
      "Epoch 2/100\n",
      "392/392 - 1s - loss: 0.3981 - auc: 0.8294 - val_loss: 0.3961 - val_auc: 0.8326\n",
      "Epoch 3/100\n",
      "392/392 - 1s - loss: 0.3906 - auc: 0.8368 - val_loss: 0.3953 - val_auc: 0.8333\n",
      "Epoch 4/100\n",
      "392/392 - 1s - loss: 0.3868 - auc: 0.8405 - val_loss: 0.3924 - val_auc: 0.8362\n",
      "Epoch 5/100\n",
      "392/392 - 1s - loss: 0.3837 - auc: 0.8436 - val_loss: 0.3918 - val_auc: 0.8367\n",
      "Epoch 6/100\n",
      "392/392 - 1s - loss: 0.3816 - auc: 0.8457 - val_loss: 0.3910 - val_auc: 0.8380\n",
      "Epoch 7/100\n",
      "392/392 - 1s - loss: 0.3794 - auc: 0.8479 - val_loss: 0.3901 - val_auc: 0.8389\n",
      "Epoch 8/100\n",
      "392/392 - 1s - loss: 0.3779 - auc: 0.8493 - val_loss: 0.3905 - val_auc: 0.8378\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "392/392 - 1s - loss: 0.3765 - auc: 0.8508 - val_loss: 0.3903 - val_auc: 0.8386\n",
      "Epoch 10/100\n",
      "392/392 - 1s - loss: 0.3720 - auc: 0.8550 - val_loss: 0.3886 - val_auc: 0.8405\n",
      "Epoch 11/100\n",
      "392/392 - 1s - loss: 0.3713 - auc: 0.8557 - val_loss: 0.3890 - val_auc: 0.8401\n",
      "Epoch 12/100\n",
      "392/392 - 1s - loss: 0.3706 - auc: 0.8563 - val_loss: 0.3894 - val_auc: 0.8397\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "392/392 - 1s - loss: 0.3703 - auc: 0.8566 - val_loss: 0.3894 - val_auc: 0.8401\n",
      "Epoch 14/100\n",
      "392/392 - 1s - loss: 0.3682 - auc: 0.8586 - val_loss: 0.3891 - val_auc: 0.8400\n",
      "Epoch 15/100\n",
      "392/392 - 1s - loss: 0.3678 - auc: 0.8590 - val_loss: 0.3890 - val_auc: 0.8403\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "392/392 - 1s - loss: 0.3675 - auc: 0.8592 - val_loss: 0.3894 - val_auc: 0.8399\n",
      "Epoch 17/100\n",
      "392/392 - 1s - loss: 0.3666 - auc: 0.8600 - val_loss: 0.3892 - val_auc: 0.8401\n",
      "Epoch 18/100\n",
      "392/392 - 1s - loss: 0.3665 - auc: 0.8602 - val_loss: 0.3891 - val_auc: 0.8402\n",
      "Epoch 19/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.\n",
      "392/392 - 1s - loss: 0.3664 - auc: 0.8602 - val_loss: 0.3891 - val_auc: 0.8401\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = baselineModel.fit(\n",
    "    x=trainX_ready,\n",
    "    y=trainY,\n",
    "    batch_size=base_batch_size,\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stop,reduce_lr_on_plateau],\n",
    "    validation_data=(validX_ready,validY),\n",
    "    shuffle=True,\n",
    "    class_weight=None,\n",
    "    sample_weight=None,\n",
    "    initial_epoch=0,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    validation_batch_size=None,\n",
    "    validation_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(trainX_ready, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
